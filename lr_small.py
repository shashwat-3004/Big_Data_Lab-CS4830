# -*- coding: utf-8 -*-
"""BDL_Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-YHucAeZ-ae4RFJOPJksbsvqlDsFfulx

# Setup

# Library Import
"""

!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash /dev/stdin -p 3.3.2 -s 4.4.0

from pyspark.context import SparkContext
from pyspark.ml.linalg import Vectors
from pyspark.sql.session import SparkSession
from pyspark.sql.types import *
from pyspark.ml import Pipeline
import sparknlp
from sparknlp.annotator import Lemmatizer, Stemmer, Tokenizer, Normalizer
from sparknlp.base import DocumentAssembler, Finisher
from pyspark.ml.feature import StopWordsRemover, CountVectorizer, IDF
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.ml.classification import DecisionTreeClassifier

spark=sparknlp.start()

data_schema = StructType([StructField('id_1', StringType(), True),
               StructField('cfu_1', FloatType(), True),
               StructField('date', StringType(), True),
               StructField('cfu_2', FloatType(), True),
               StructField('id_2', StringType(), True),
               StructField('stars', FloatType(), True),
               StructField('text', StringType(), True),
               StructField('cfu_3', FloatType(), True),
               StructField('id_3', StringType(), True)])

yelp_dataset = spark.read.format("csv").schema(data_schema)\
.option("mode", "DROPMALFORMED")\
.option("quote", '"')\
.option("multiline", "true")\
.option("escape", "\"")\
.load(["/content/drive/MyDrive/YELP_train.csv_part-00001-808f9971-b2b6-4a6f-b8cf-0822a68f365f-c000.csv",
        "/content/drive/MyDrive/YELP_train.csv_part-00000-808f9971-b2b6-4a6f-b8cf-0822a68f365f-c000 (1).csv"])

yelp_dataset.printSchema()

yelp_dataset.createOrReplaceTempView("yelp")

yelp_dataset=yelp_dataset.filter((yelp_dataset.stars>0)& (yelp_dataset.stars<=5))
yelp_dataset = yelp_dataset.filter("stars is NOT NULL AND text is NOT NULL") 
print('the final Yelp dataset has ' + str(yelp_dataset.count()) + ' rows.')

training_data, test_data = yelp_dataset.randomSplit([0.8, 0.2], seed = 42)

document_assembler = DocumentAssembler().setInputCol("text").setOutputCol("document")
tokenizer = Tokenizer().setInputCols(["document"]).setOutputCol("token")
normalizer = Normalizer().setInputCols(["token"]).setOutputCol("normal")
stemmer = Stemmer().setInputCols(["normal"]).setOutputCol("stem")
finisher = Finisher().setInputCols(["stem"]).setOutputCols(["to_spark"]).setValueSplitSymbol(" ")
stopword_remover = StopWordsRemover(inputCol = "to_spark", outputCol = "filtered")
tf = CountVectorizer(inputCol = "filtered", outputCol = "raw_features")
idf = IDF(inputCol = "raw_features", outputCol = "features")



lr = LogisticRegression(featuresCol = 'features', labelCol = 'stars',maxIter=10)

pipe = Pipeline(
	stages = [document_assembler, tokenizer, normalizer, stemmer, finisher, stopword_remover, tf, idf, lr]
)

print("Fitting the model to the data")
lr_model = pipe.fit(training_data)
print('Training done.')

lr_evaluator_f1 = MulticlassClassificationEvaluator(predictionCol = "prediction", 
                                  labelCol = "stars", metricName = "f1")

lr_evaluator_acc= MulticlassClassificationEvaluator(predictionCol = "prediction", 
                                  labelCol = "stars", metricName = "accuracy")
# training prediction
prediction_train = lr_model.transform(training_data)
prediction_train.select("prediction", "stars", "features").show(5)
print("Evaluating on Training data(F1):", lr_evaluator_f1.evaluate(prediction_train))
print("Evaluating on Training data(Accuracy):", lr_evaluator_acc.evaluate(prediction_train))

prediction_test = lr_model.transform(test_data)
prediction_test.select("prediction", "stars", "features").show(5)

print("F1:", lr_evaluator_f1.evaluate(prediction_test))
print("Accuracy:", lr_evaluator_acc.evaluate(prediction_test))