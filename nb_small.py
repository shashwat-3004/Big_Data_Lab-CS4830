# -*- coding: utf-8 -*-
"""BDL_Project: Hrishabh

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18q1FgVSAH0nahY1SKDTrsJAnSDdENVCZ

# Setup
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Library Import"""

!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash /dev/stdin -p 3.3.2 -s 4.4.0

from pyspark.context import SparkContext
from pyspark.ml.linalg import Vectors
from pyspark.sql.session import SparkSession
from pyspark.sql.types import *
from pyspark.ml import Pipeline
import sparknlp
from sparknlp.annotator import Lemmatizer, Stemmer, Tokenizer, Normalizer
from sparknlp.base import DocumentAssembler, Finisher
from pyspark.ml.feature import StopWordsRemover, CountVectorizer, IDF
from pyspark.ml.classification import NaiveBayes
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder

#spark.stop()

spark=sparknlp.start()

"""## Reading the file"""

file1_location = "/content/drive/MyDrive/BDL_Project/YELP_train.csv_part-00000-808f9971-b2b6-4a6f-b8cf-0822a68f365f-c000.csv"
file2_location = "/content/drive/MyDrive/BDL_Project/YELP_train.csv_part-00001-808f9971-b2b6-4a6f-b8cf-0822a68f365f-c000.csv"

data_schema = StructType([StructField('id_1', StringType(), True),
               StructField('cfu_1', FloatType(), True),
               StructField('date', StringType(), True),
               StructField('cfu_2', FloatType(), True),
               StructField('id_2', StringType(), True),
               StructField('stars', FloatType(), True),
               StructField('text', StringType(), True),
               StructField('cfu_3', FloatType(), True),
               StructField('id_3', StringType(), True)])

yelp_dataset = spark.read.format("csv").schema(data_schema)\
.option("mode", "DROPMALFORMED")\
.option("quote", '"')\
.option("multiline", "true")\
.option("escape", "\"")\
.load([file1_location, file2_location])

yelp_dataset = yelp_dataset.filter("stars is NOT NULL AND text is NOT NULL") 
print('the final Yelp dataset has ' + str(yelp_dataset.count()) + ' rows.')

training_data, test_data = yelp_dataset.randomSplit([0.8, 0.2], seed = 42)

yelp_dataset.filter("text is NOT NULL").count()

yelp_dataset.select("text").show(5)

"""## Text Preprocessing"""

from pyspark.ml import Pipeline

document_assembler = DocumentAssembler().setInputCol("text").setOutputCol("document")
tokenizer = Tokenizer().setInputCols(["document"]).setOutputCol("token")
normalizer = Normalizer().setInputCols(["token"]).setOutputCol("normal")
stemmer = Stemmer().setInputCols(["normal"]).setOutputCol("stem")
finisher = Finisher().setInputCols(["stem"]).setOutputCols(["to_spark"]).setValueSplitSymbol(" ")
stopword_remover = StopWordsRemover(inputCol = "to_spark", outputCol = "filtered")
tf = CountVectorizer(inputCol = "filtered", outputCol = "raw_features")
idf = IDF(inputCol = "raw_features", outputCol = "features")

nb = NaiveBayes(featuresCol = 'features', labelCol = 'stars')

pipe = Pipeline(
	stages = [document_assembler, tokenizer, normalizer, stemmer, finisher, stopword_remover, tf, idf, nb]
)

nb_model = pipe.fit(training_data)

nb_evaluator = MulticlassClassificationEvaluator(predictionCol = "prediction", 
                                  labelCol = "stars", metricName = "f1")

# training prediction
prediction_train = nb_model.transform(training_data)
prediction_train.select("prediction", "stars", "features").show(5)
print("Evaluating on Training data:", nb_evaluator.evaluate(prediction_train))

prediction_test = nb_model.transform(test_data)
print("F1:", nb_evaluator.evaluate(prediction_test))